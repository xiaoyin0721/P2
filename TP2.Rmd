---
title: "P2"
author: "Tobias Pedersen, Dat Luong, Adam Rumi, Kristoffer Lading, Xiaoyin Chang, Kasper Sommer"
date: "4/3/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(mosaic)
library(ggplot2)
library(cowplot)
require(gridExtra)
#tinytex::install_tinytex()
```


# Introduktion
I denne del af projektet, skabes der en Pseudo-Random Number generator, hvis formål er at generere tilfældige tal. Fordelingen af disse tal vil være uniform, og ved hjælp af en Box-Muller transformation vil der opnås en normalfordeling. Grunden til dette er at undersøge stikprøver fra en normalfordeling i forhold til Central Limit Theorem, da der er adskillige interessante statistiske spørgsmål herinden under.


# Pseudo-Random Number Generators
For at kunne generere tilfældige tal ud fra deterministiske computere,er det nødvendigt at bearbejde et input ved hjælp af en algoritme, således at det genererede tal tilsyneladende er tilfældigt. Et sådanne genereret tal kaldes et "Pseudo-Random Number", og disse bliver genereret ved hjælp af Pseudo-Random Number Generators (også kaldet PRNG'er). PRNG'er benytter et seed, som kan bestemmes af brugeren, til at generere de tilfældige tal. Et eksempel på en kendt PRNG, er en Lineære Kongruentiel Generator (Også kaldet LCG). 

# True-Random Number Generators
I modsætning til PRNG'er, eksisterer der også True-Random Number Generators (også kaldet TRNG'er). Disse generators genererer tilfældige tal, uden nødvendigvis at afhænge af algoritmer. Et eksempel på en kendt TRNG, er at generere tilfældige tal ved hjælp af atmosfærisk støj.

# Sammenligning af PRNG'er og TRNG'er

En PRNG er meget velegnet til simulationer, da det er muligt at reproducere dataet ved at sætte seed'et til en bestemt værdi. Dette gør det muligt for andre indenfor samme område, at kunne få dataet fra en given simulation, og derved er det nemmere at diskutere fund og eller problemer med den givne simulation.

TRNG'er kan også benyttes til simulationer, og siden tallene er mere tilfældige i forhold til PRNG'erne, kan de være mere velegnet til simulationer. Problemet er dog, at siden seedets værdi ikke kan bestemme, er det umuligt at reproducere dataet. Grundet dette, er PRNG i adskillige situationer foretrukket indenfor simulationer. Dog er det værd at nævne, at siden dataet fra en TRNG ikke kan reproduceres, er det optimalt for ting som skal være tilfældige, såsom lotteriet, gambling eller kryptering.

I dette projekt vil en PRNG implementeres i R, hvornæst de uniform fordelte tal skal transformeres v.h.a. Box-Muller. Dernæst vil R's indbyggede PRNG benyttes til at undersøge adskillige statistiske spørgsmål.

# Lineær kongruens generator
I en lineær kongruens generator (LCG) er det muligt at generere tilfældige tal. LCG er en PRNG, så tallene der fås fra generatoren, vil ikke være fuldstændigt tilfældige. For LCG'en benyttes følgende formel, for at generere de tilfældige tal: 
$$
\begin{aligned}
      X_{(n+1)}=(a*X_n+c)\;mod\;m
\end{aligned}      
$$
	- $X_0$, som svarer til denne generators seed, $X_0$ $\geq$ 0.  
	- $a$, som bliver ganget på $X_0$, $a$ $\geq$ 0.  
	- $c$, som bliver adderet til $X_0$, $c$ $\geq$ 0.  
	- $m$, kaldet modulus, $m > X_0$, $m > a$, $m > c$.  

Her vil man starte med at indsætte $X_0$ på $X_{n}$'s plads, og ud fra dette kan man finde $X_1$. Derefter kan man indsætte $X_1$ på $X_n$’s plads og derefter få $X_2$. Denne proces kan gentages så mange gange som man har brug for.
Det er vigtigt at nævne, at før eller siden vil tallene fra sådanne en generator begynde at gentage sig selv, længden fra det første tal i generatoren frem til det første gentagende tal kaldes for en periode, og perioden afhænger meget af de valgte værdier af $a$, $c$ og $m$. Ved at ændre m til et meget højt tal, vil der dog gå meget lang tid før at tallene begynder at gentage sig selv. Andre der bruger denne generator anbefaler $2^{31}$. 
De tilfældige tal man kan få ud af denne generator, er uniform fordelt. Tallene er kontinuære. Ved at omhyggeligt vælge sine $a, c$ og $m$ værdier kan man også sørge for at tallene man får, ikke ser ud til at have nogen korrelation med hinanden. GØR DET HER TOBIAS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! 

```{r}
#![](C:/Users/tobop/OneDrive - Aalborg Universitet/Documents/GitHub/P2/PRNG_Forklaring.jpg){width=50%}
```
Når ens program startes op får man en state ud fra det seed man bruger. Denne state kan dernæst ændres til en anden state ved hjælp af en funktion f, som ikke har en invers funktion. Dette kan dog kun gøres en gang per tilstand, hvorimod andre PRNG’er kan have flere tilstande på en gang. Et eksempel er Mersenne twister, men denne vil ikke gennemgås i rapporten.

# Implementering af linear congruential generator
Nedenstående blok kode blev brugt til at lave den lineære kongruentiale generator (LCG):
```{r}

linear_congruence <- function(i, X_0) {
t <- 0
a <- 11102357
c <- 21353
m <- 2**32
v1 <- c()
while(t < i){
  X_0 <- (a*X_0+c)%%m
  v1 <- c(v1 , as.numeric(((X_0)/m)))
  t <- t + 1
}
return(v1)
}
linear_congruence(i=10, X_0 = 234)
```

Som beskrevet i afsnittet om LCG'er, skal generatoren bruge et seed. Derfor sættes $X_0$ lig med et tilfældigt tal, som i dette tilfælde er $1576$. Der skulle også bruges en $a$, $c$ og $m$ værdi, som i denne kode er sat til henholdsvis $11,102,357$, $21,353$ og $2^{32}$. Der er også oprettet to variabler: $t$ og $i$, som bruges i programmet, hvor $i$ er en parameter for funktionen, som svarer til det ønskede antal tilfældigt genererede tal, og $t$ har en startsværdi på 0, og øges med 1, hvert gang et tilfældigt tal genereres. Funktionen er sat op således, at så længe $t$ er mindre end $i$, vil $X_0$ blive brugt til at udregne en ny $X_0$ værdi. Værdien bliver dernæst tilføjet til en vektor $v1$, dog ikke før at værdien bliver divideret med $m$ og ganget med 100. Dernæst bliver $1$ adderet til $t$, og så loop'er funktionen. Resultatet af denne funktion, kan ses forneden:  


```{r Linear Congruence, echo=FALSE}

gf_histogram(~linear_congruence(i=100000, X_0 = 1576), breaks = seq(0, 1, by=0.1), fill="black", col="grey", xlim = c(0, 1), ylab = "Antal", xlab = "Tilfældige tal inddelt i intervaller",title = "Resultaterne af den implementerede PRNG")
    
```

# Box-Muller transformation
Box-Muller transformationen er en metode, hvori to uniforme random variabler transformeres til en normal fordeling. Den primære ide er at ændre fra koordinatorne fra kartetiske til polære koordinator. 

The principle idea is..instead of sampling independent Gaussians for the x and y coordinates of the samples, we sample their independent uniform angles and exponential squared distances to origin.

For two independent random variables x and y, the joint probability density function $f(x,y)$ is simply the product of density functions f(x) and f(y). 


Sammenhænget mellem kartetiske og polære-koordinator er:
$x = cos(\theta)*r$
$y = sin(\theta)*r$

By writing the joint density function in polar cooridnates, 'the joint PDF is the product of a constant $1/2\pi$ and an exponential decay of $r^2$ with the parameter $\lambda = 1/2$. 

In pratices, $U_1$ and $U_2$ are uniform random variables, $R^2$ is calculated by applying the inverse CDF method to sample from an exponential decay distribution with $\lambda = 1/2$, which is $-2ln(U_1)$. $\Theta$ is calculated from muliply $2\pi$ on $U_2$


Dette svarer til at generere en tilfærdig vikel og en tilfældig radius, som følger en ekponential fordeling, which means it is more likely to generate distances closer to the origin, which is shown as


```{r}
uni_rand_num1 <- linear_congruence(i=10000, X_0 = 145)
uni_rand_num2 <- linear_congruence(i=10000, X_0 = 7346)
R <- sqrt(-2*log(uni_rand_num1))
theta <- 2*pi*uni_rand_num2
X <- R*cos(theta)
Y <- R*sin(theta)

plot(X,Y,pch=19, cex=0.4, asp=1, las=1)


```



#Quantile-quantile plot

A Q–Q plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other. we use qqplot to check the generated data X, Y follows a normal distribution. 

The result is most data points fall along a straight line, which means data points of X,Y appears to be normally distributed. 


```{r}


qqplot(X,Y)


```

#Uafhængig Test 




```{r}

```







Ud af dette, fås der to tal $x$ og $y$, som i dette tilfælde er uafhængige tilfældige variabler med en normalfordeling. Forneden kan resultaterne af Box-Muller transformationen ses:



```{r echo=FALSE}

Box_muller_transform <- function(){
  n <- 10^4
  samples <- matrix(ncol = 2, nrow = n)
  uni_rand_num1 <- linear_congruence(i=10000, X_0 = 145)
  uni_rand_num2 <- linear_congruence(i=10000, X_0 = 7346)
  R <- sqrt(-2*log(uni_rand_num1))
  theta <- 2*pi*uni_rand_num2
  X <- R*cos(theta)
  Y <- R*sin(theta)
  samples[,1] <- X
  samples[,2] <- Y 
  
  Label <- rep(c("x", "y"),n)
  value <- c(samples[,1],samples[,2])
  df <- data.frame(value, Label)
  library(ggplot2)
  plt <- ggplot(df, aes(x=value, color=Label, fill=Label)) + geom_histogram(aes(y=..density..), bins = 60, position= "identity", alpha =0.3) + labs(title = "Box-Muller Transformationen", x="Value", y="Density", ) + theme_bw()
  print(plt)
}
Box_muller_transform()


```



# Teori

## Middelværdi og standardafvigelse
I en population findes der to værdier, $\mu$ og $\sigma$, som er interessante at kigge nærmere på. Gennemsnittet af en population kaldes $\mu$, mens at populationens standardafvigelse kaldes $\sigma$.

Populations standardafvigelse kan udregnes ud fra populationens varians. Nedenstående ligning viser hvordan variansen for en stikprøve kan udregnes:


$$
\begin{aligned}
var(x)=s^2 = \sum_{i=1}^{n} \frac{(x_i-\overline{x})^2}{n}
\end{aligned}
$$
Herunder er der:
- $x_i$, som svarer til den enkelte observation i stikprøven.
- $\overline{x}$, som svarer til gennemsnittet af stikprøven.

Sammenhænget mellem en populations standardafvigelse og populationens varians kan ses i nedenstående ligning.:
$$
\begin{aligned}
\sigma=\sqrt{var(X)}=\sqrt{\sigma^2}
\end{aligned}
$$
Populationen bliver her betegnet som X.

## Fordelinger
Der findes en lang række sandynlighedsfordelinger og i det følgende afsnit fokuseres der på de fordelinger, som har mest relevans for projektet. 
### Standardnormalfordeling
- Billede: 
En standardnormalfordeling er kendetegnet ved at $$\mu=0$$, og at $$\sigma = 1$$. Desuden har en standardnormalfordeling et klokkeformet udseende. Nedenstående er der plottet en standardnormalfordeling:
```{r}
qdist("norm",p=1,mean = 0, sd = 1,xlim = c(-4,4))
```
Der er en høj sandsynlighed for at få et værdi tæt på 0. Jo længere man afviger fra $$\mu$$, jo mindre er densiteten,som medfører at sandsynligheden for de givne værdier mindskes. Ergo vil sandsynligheden for at få 2, være mindre end sansynligheden for at få 0. 
En af anveldeserne for en standarnormalfordeling er at den kan benyttes til en Z-test, som benyttes senere i projektet.


####Z-fordeling
For at finde z-scoren, skal $$\mu,\sigma$$ være kendt. 
Z-scoren kan udregnes for normalfordelinger ved formlen:
$$\begin{aligned}
  z = \frac{y-\mu}\sigma
  \end{aligned}$$
,hvor $y$ er en værdi, som er $z$ standardafvigelser fra $\mu$
Desuden, kan det udledes, at hvis $z>0$ er y-værdien på højre side af $\mu$, og omvendt, hvis  $z<0$, er y-værdien på venstre side af $\mu$.

####T-fordeling
En T-fordeling laves ud fra en standardnormalfordeling og dens udseende minder også om denne fordeling. Forskellen på en T-fordeling og en standardnormalfordeling, ligger i, at en T-fordeling laves ud fra frihedsgrader. For en stikprøve vil antallet af frihedsgrader være antallet af observationer minus 1, altså $n-1$. Jo flere frihedsgrader der er, jo mere ligner en T-fordeling en standardnormalfordeling. Nedenstående er der plottet 2 T-fordelinger med henholdsvis 5 og 100 frihedsgrader:
```{r, fig.height = 3, fig.width = 8} 
p1 <- qdist("t", df = 5, p = 1, return = "plot")
p2 <- qdist("t", df = 100, p = 1, return = "plot")
#p3 <- qdist(mean = 0, sd = 1, p = 0, return="plot")
grid.arrange(p1, p2, ncol=2)
```


####T-fordeling
En T-fordeling laves ud fra en standardnormalfordeling og dens udseende minder også om denne fordeling. Forskellen på en T-fordeling og en standardnormalfordeling, ligger i, at en T-fordeling laves ud fra frihedsgrader. For en population vil antallet af frihedsgrader være antallet af observationer minus 1, altså $n-1$. Jo flere frihedsgrader der er, jo mere ligner en T-fordeling en standardnormalfordeling.  

### Binomialfordeling
Binomialfordelingen benyttes i sammenhænge med kvalitative data. Specifikt er der kun to mulige udfald. Hvis der er lige stor chance for hvert af udfaldene, altså $ \pi = 0.50$ , og en stor stikprøve, vil en binomialfordeling vise sig at være symmetrisk.
Der tages et eksempel, som simulerer plat eller krone. 

```{r}
set.seed(987)
rbino1 <- c(rbinom(100, 20, 0.5))
p1 <- gf_histogram(~ rbino1, fill="black", col="grey", breaks = seq(2,18, by=1)) #Test 1: Grundtest,(20 mønter kastes 100 gange)


#Test 2: Antallet af test forøges
set.seed(987)
rbino2 <- c(rbinom(10000,20,0.5))
p2 <- gf_histogram(~ rbino2, fill = "black", col = "grey", breaks = seq(2,18, by =1)) 

#Test 3: Antallet af kroner forøges
set.seed(987)
rbino3 <- c(rbinom(100,30,0.5))
p3 <- gf_histogram(~ rbino3, fill = "black", col = "grey", breaks = seq(5,25, by =1)) 

#Test 4: Sandsynligheden ændres fra 50%
set.seed(987)
rbino4 <- c(rbinom(100,20,0.3))
p4 <- gf_histogram(~ rbino4, fill = "black", col = "grey", breaks = seq(0,11, by =1)) 

grid.arrange(p1, p2, p3, p4,ncol=2,nrow = 2)

```

$$ \hat\pi = \frac{x}n$$




- 100 mønter
- kvalitativ    

### Poisson Fordeling


### Uniform fordeling
En uniform fordeling, er hvor dataet bliver indelt i lige store intervaller, og alle intervaller, har lige antal observationer. Det er vigtigt at notere at hvis antallet af observationer er lavt, vil fordelingen ikke tilsyneladende være uniform. Dette vises ved at lave to uniformfordelinger på det samme seed,hvor den eneste forskel er antallet af observationer. Dette kan ses i de nedenstående grafer. 

```{r, fig.height=7}

set.seed(1234)
unifx <- runif(50,min = 0, max =1)
set.seed(1234)
unifx1 <- runif(5000, min = 0, max = 1)

p1 <- gf_histogram(~unifx,breaks = seq(0,1,by=0.1), fill="black", col="grey",xlab = "tilfældige tal inddelt i intervaller", ylab = "antal", title = "Uniform fordeling ved 50 observationer")
p2 <- gf_histogram(~unifx1,breaks = seq(0,1,by=0.1), fill="black", col="grey",xlab = "tilfældige tal inddelt i intervaller", ylab = "antal", title = "Uniform fordeling ved 5000 observationer")
grid.arrange(p1, p2, nrow=2)
```


## CLT


## Statistisk inferens

### Estimation

### Konfidensinterval

Når der indenfor statistik estimeres på en populations parameter ud fra en stikprøve, vil resultatet aldrig være perfekt. Dette skyldes at der er en fejlmargin, som der skal tages højde fra. En måde hvorpå man kan tage højde for denne fejlmargin er et konfidensinterval.

Et konfidensinterval for en givet parameter er et interval mellem to tal, hvori det estimeres at parameteren ligger i.
Sandsynligheden for at producere et interval som indeholder parameteren kaldes for et konfidensniveau. Denne værdi er valgt til et tal tæt på 1, som regel enten 0.95 eller 0.99.

En konfidensinterval skabes på baggrund af et punktestimat og fejlmarginen. Måden dette gøres på, kan ses forneden:
$$
\begin{aligned}
KI = Punktestimat \; \pm \; fejlmargin
\end{aligned}
$$
Måden at finde konfidensintervallet varierer baseret på hvilket fordeling man bruger. I de følgende afsnit vil der forklares hvordan konfidensintervallet for kvalitative og kvantitative variabler findes og opstilles ved hjælp af eksempler.

#### Kvalitative variabler

Hvis man spurgte en population hvorvidt de kunne lide deres job eller ej, ville en andel svare "ja" og en andel svare "nej". Er man interesseret i hvor mange procent svarede "ja" til spørgsmålet, ville man dividere antallet der svarede "ja" med antallet af observation. Se følgende:
$$
\begin{aligned}
\hat{\pi}_{ja} = \frac{n_{ja}}{n{_{total}}}
\end{aligned}
$$
Før selve konfidensintervallet kan opstilles, skal standardfejlen ($sf$) findes. Følgende formel bruges for at finde standardfejlen for kvalitative variabler:
$$
\begin{aligned}
sf(\hat{\pi}) = \sqrt{\frac{\hat{\pi}(1-\hat{\pi})}n{}}
\end{aligned}
$$
Når standardfejlen er fundet, kan denne ganges med en værdi som afhænger af signifikansniveauet. Denne værdi kaldes $z_{crit}$. For at finde ud af hvilken værdi $z_{crit}$ skal være, benyttes en z-test. En z-test er i realiteten bare en standardnormalfordeling. For at finde $z_{crit}$ for signifikansniveauet 95%, findes x-værdien ved 97.5% og 2.5%.
```{r}
qdist("norm", mean = 0, sd = 1, p = c(0.025,0.975))
```
Denne værdi aflæses til at være ~1.96. Nu kan konfidensintervallet opstilles via følgende ligning:
$$
\begin{aligned}
KI = \hat{\pi}\; \pm 1.96\cdot sf(\hat{\pi})
\end{aligned}
$$
Da dette blev lavet udfra signifikansniveauet 95%, kan der altså med 95%'s sikkerhed siges at $\hat\pi$ ligger indenfor intervallet:
$$
\begin{aligned}
(\hat\pi\;-1.96\;\cdot sf(\hat\pi) \ \ ,\ \  \hat\pi \; + 1.96\;\cdot sf(\hat\pi))
\end{aligned}
$$

#### Kvantitative variabler

Konfidensintervallet for kvantitative variabler har næsten samme formel som ved kvalitative variabler. Dog er måden hvorpå standardfejlen findes anderledes. Desuden skal der bruges en anden slags test ved hjælp af signifikansniveauet. 

$$
\begin{aligned}
KI = \overline{y}\;\pm t_{crit} \;\cdot sf(\overline{y})
\end{aligned}
$$
Herunder er standardfejlen udregnet ved: 
$$
\begin{aligned}
sf(\overline{y}) = \frac{s}{\sqrt{n}}
\end{aligned}
$$
hvor s er standardafvigelsen for stikprøven og n er antallet af observationer i stikprøven.

For at finde $t_{crit}$ benyttes der en t-test. Hvis det ønskede signifikansniveau igen er 95% hvor der igen aflæses x-værdien ved $2.5\%$ og $97.5\%$. Nedenstående er der et eksempel, hvor en t-test med 50 frihedsgrader benyttes til at finde 95% signifikansniveauet:
```{r}
qdist("t", df = 50, p = c(0.025,0.975))
```
Denne værdi aflæses til at være ~2.01. Nu kan konfidensintervallet igen opstilles:
$$
\begin{aligned}
(\overline{x}\;-2.01 \;\cdot sf(\overline{x}) \ \ , \ \ \overline{x}\;+2.01 \;\cdot sf(\overline{x})
\end{aligned}
$$

### Hypoteser
Hver signifikanstest har to hypoteser. En hypotese er et udsagn omkring værdien af population parameter. Hypotese forudsiger hvorvidt en parameter påvirker en numerisk værdi eller falder inden for en specifik værdimængde. Det gør det så muligt at beregne om en hypotese er sand eller falsk.  

For at sørge for at man anvender en pålidelig analysemetode på en stikprøve, så opsættes en hypotese omkring sammenhængen af ens data så opstilles en hypotesetest. Det er en metode til at teste en hypotese (Alternativ hypotese - $H_a$) hvor den sættes op mod en konkurrence hypotese som er nulhypotesen ($H_0$). En stikprøve tages ud af en population, hvor $H_0$ testes imod $H_a$ ved brug af et signifikansniveau ($\alpha$) og en $p$-værdi.

Et klassisk eksempel på dette kunne være at opstille:
$$
\begin{aligned}
H_0 : \mu = \mu_0
\\
H_a : \mu \not= \mu_0
\end{aligned}
$$
Hvor $\mu$ er den ene stikprøves populations middelværdi og $\mu_0$ er er den andens stikprøves populations middelværdi.

For at tjekke hvilken af hypoteserne er sand eller falsk, vælges et signifikansniveau som ofte er $\alpha$ = 5% eller $\alpha$ = 1%. Ud fra dette signifikansniveau bliver der udregnet en kritisk værdi på en sandsynlighedsfordeling.

$p$ > $\alpha$ så forkastes man ikke $H_0$.
\
$p$ $\le$ $\alpha$ så forkastes $H_0$ imod $H_1$. 

For at finde $p$-værdien skal der foretages en hypotesetest. Der findes en række hypotesetest, som vil være relevante i forskellige sammenhænge.

# Metoder


```{r, fig.height = 3, fig.width = 8} 
p1 <- qdist(mean = 0, sd = 1, p = 1, return = "plot")
p2 <- qdist(mean = 0, sd = 1, p = 0.5, return = "plot")
#p3 <- qdist(mean = 0, sd = 1, p = 0, return="plot")
grid.arrange(p1, p2, ncol=2)
```